{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C4_W3_1_Question_Answering_with_BERT_and_HuggingFace_Pytorch_tydiqa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedahmmednorthsouth/Natural-Language-Processing-with-Attention-Models/blob/main/C4_W3_1_Question_Answering_with_BERT_and_HuggingFace_Pytorch_tydiqa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2UXutvEvpUj"
      },
      "source": [
        "# Question Answering with BERT and HuggingFace\n",
        "\n",
        "You've seen how to use BERT, and other transformer models for a wide range of natural language tasks, including machine translation, summarization, and question answering. Transformers have become the standard model for NLP, similar to convolutional models in computer vision. And all started with Attention!\n",
        "\n",
        "In practice, you'll rarely train a transformer model from scratch.  Transformers tend to be very large, so they take time, money, and lots of data to train fully. Instead, you'll want to start with a pre-trained model and fine-tune it with your dataset if you need to.\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) (ðŸ¤—) is the best resource for pre-trained transformers. Their open-source libraries simplify downloading and using transformer models like BERT, T5, and GPT-2. And the best part, you can use them alongside either TensorFlow, PyTorch and Flax. \n",
        "\n",
        "In this notebook, you'll use ðŸ¤—  transformers to download and use the DistilBERT model for question answering. \n",
        "\n",
        "First, let's install some packages that we will use during the lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rW5HyNyv3YC",
        "outputId": "264cf26d-0c21-4f5c-92dc-b464419d1788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 51.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 895 kB 43.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.5 MB 32.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm675LmQvpUm"
      },
      "source": [
        "## Pipelines\n",
        "\n",
        "Before fine-tuning a model, you will look to the pipelines from Hugging Face\n",
        "to use pre-trained transformer models for specific tasks. The `transformers` library provides pipelines for popular tasks like sentiment analysis, summarization, and text generation. A pipeline consists of a tokenizer, a model, and the model configuration. All these are packaged together into an easy-to-use object. Hugging Face makes life easier. \n",
        "\n",
        "Pipelines are intended to be used without fine-tuning and will often be immediately helpful in your projects. For example, `transformers` provides a pipeline for [question answering](https://huggingface.co/transformers/main_classes/pipelines.html#the-pipeline-abstraction) that you can directly use to answer your questions if you give some context. Let's see how to do just that.\n",
        "\n",
        "You will import `pipeline` from `transformers` for creating pipelines. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "git remote add origin https://github.com/syedahmmednorthsouth/Natural-Language-Processing-with-Attention-Models.git\n",
        "git branch -M main\n",
        "git push -u origin main"
      ],
      "metadata": {
        "id": "iMQ6tyG6a9RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "id": "7yrp6YilZa_I",
        "outputId": "63e428ea-a990-4ee6-a948-866edd455567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "bJQcaFlKZiP3",
        "outputId": "056624d5-75c4-4a10-e029-3c6e9aa16f6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "\n",
            "No commits yet\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNJGGbRWvpUm"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CeFTIr7P3QR"
      },
      "source": [
        "Now, you will create the pipeline for question-answering, which uses the [DistilBert](https://hf.co/distilbert-base-cased-distilled-squad) model for extractive question answering (i.e., answering questions with the exact wording provided in the context). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKy4AAhLvpUo"
      },
      "source": [
        "# The task \"question-answering\" will return a QuestionAnsweringPipeline object\n",
        "question_answerer = pipeline(task=\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltQLVWgvpUo"
      },
      "source": [
        "After running the last cell, you have a pipeline for performing question answering given a context string. The pipeline `question_answerer` you just created needs you to pass the question and context as strings. It returns an answer to the question from the context you provided. For example, here are the first few paragraphs from the [Wikipedia entry for tea](https://en.wikipedia.org/wiki/Tea) that you will use as the context.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_-MzZNJvpUp"
      },
      "source": [
        "context = \"\"\"\n",
        "Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis,\n",
        "an evergreen shrub native to China and East Asia. After water, it is the most widely consumed drink in the world. \n",
        "There are many different types of tea; some, like Chinese greens and Darjeeling, have a cooling, slightly bitter, \n",
        "and astringent flavour, while others have vastly different profiles that include sweet, nutty, floral, or grassy \n",
        "notes. Tea has a stimulating effect in humans primarily due to its caffeine content.\n",
        "\n",
        "The tea plant originated in the region encompassing today's Southwest China, Tibet, north Myanmar and Northeast India,\n",
        "where it was used as a medicinal drink by various ethnic groups. An early credible record of tea drinking dates to \n",
        "the 3rd century AD, in a medical text written by Hua Tuo. It was popularised as a recreational drink during the \n",
        "Chinese Tang dynasty, and tea drinking spread to other East Asian countries. Portuguese priests and merchants \n",
        "introduced it to Europe during the 16th century. During the 17th century, drinking tea became fashionable among the \n",
        "English, who started to plant tea on a large scale in India.\n",
        "\n",
        "The term herbal tea refers to drinks not made from Camellia sinensis: infusions of fruit, leaves, or other plant \n",
        "parts, such as steeps of rosehip, chamomile, or rooibos. These may be called tisanes or herbal infusions to prevent\n",
        "confusion with 'tea' made from the tea plant.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyR3o2mrvpUq"
      },
      "source": [
        "Now, you can ask your model anything related to that passage. For instance, \"Where is tea native to?\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiRohAWWvpUq",
        "scrolled": true
      },
      "source": [
        "result = question_answerer(question=\"Where is tea native to?\", context=context)\n",
        "print(result['answer'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRXzFlZ5vpUr"
      },
      "source": [
        "You can also pass multiple questions to your pipeline within a list so that you can ask:\n",
        "\n",
        "*   \"Where is tea native to?\"\n",
        "*   \"When was tea discovered?\"\n",
        "*   \"What is the species name for tea?\"\n",
        "\n",
        "at the same time, and your `question-answerer` will return all the answers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMLyXeMZvpUr"
      },
      "source": [
        "questions = [\"Where is tea native to?\",\n",
        "             \"When was tea discovered?\",\n",
        "             \"What is the species name for tea?\"]\n",
        "\n",
        "results = question_answerer(question=questions, context=context)\n",
        "\n",
        "for q, r in zip(questions, results):\n",
        "    print(q, \"\\n>> \" + r['answer'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXf18tVu8p70"
      },
      "source": [
        "Although the models used in the Hugging Face pipelines generally give outstanding results, sometimes you will have particular examples where they don't perform so well. Let's use the following example with a context string about the Golden Age of Comic Books:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v9C0TAqwinw"
      },
      "source": [
        "context = \"\"\"\n",
        "The Golden Age of Comic Books describes an era of American comic books from the \n",
        "late 1930s to circa 1950. During this time, modern comic books were first published \n",
        "and rapidly increased in popularity. The superhero archetype was created and many \n",
        "well-known characters were introduced, including Superman, Batman, Captain Marvel \n",
        "(later known as SHAZAM!), Captain America, and Wonder Woman.\n",
        "Between 1939 and 1941 Detective Comics and its sister company, All-American Publications, \n",
        "introduced popular superheroes such as Batman and Robin, Wonder Woman, the Flash, \n",
        "Green Lantern, Doctor Fate, the Atom, Hawkman, Green Arrow and Aquaman.[7] Timely Comics, \n",
        "the 1940s predecessor of Marvel Comics, had million-selling titles featuring the Human Torch,\n",
        "the Sub-Mariner, and Captain America.[8]\n",
        "As comic books grew in popularity, publishers began launching titles that expanded \n",
        "into a variety of genres. Dell Comics' non-superhero characters (particularly the \n",
        "licensed Walt Disney animated-character comics) outsold the superhero comics of the day.[12] \n",
        "The publisher featured licensed movie and literary characters such as Mickey Mouse, Donald Duck,\n",
        "Roy Rogers and Tarzan.[13] It was during this era that noted Donald Duck writer-artist\n",
        "Carl Barks rose to prominence.[14] Additionally, MLJ's introduction of Archie Andrews\n",
        "in Pep Comics #22 (December 1941) gave rise to teen humor comics,[15] with the Archie \n",
        "Andrews character remaining in print well into the 21st century.[16]\n",
        "At the same time in Canada, American comic books were prohibited importation under \n",
        "the War Exchange Conservation Act[17] which restricted the importation of non-essential \n",
        "goods. As a result, a domestic publishing industry flourished during the duration \n",
        "of the war which were collectively informally called the Canadian Whites.\n",
        "The educational comic book Dagwood Splits the Atom used characters from the comic \n",
        "strip Blondie.[18] According to historian Michael A. Amundson, appealing comic-book \n",
        "characters helped ease young readers' fear of nuclear war and neutralize anxiety \n",
        "about the questions posed by atomic power.[19] It was during this period that long-running \n",
        "humor comics debuted, including EC's Mad and Carl Barks' Uncle Scrooge in Dell's Four \n",
        "Color Comics (both in 1952).[20][21]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYbERLKQbhyH"
      },
      "source": [
        "Let's ask the following question: \"What popular superheroes were introduced between 1939 and 1941?\" The answer is in the fourth paragraph of the context string. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEmAbSSGbg0J"
      },
      "source": [
        "question = \"What popular superheroes were introduced between 1939 and 1941?\"\n",
        "\n",
        "result = question_answerer(question=question, context=context)\n",
        "print(result['answer'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGx_BHkN-ejY"
      },
      "source": [
        "Here, the answer should be: \n",
        "\"Batman and Robin, Wonder Woman, the Flash, \n",
        "Green Lantern, Doctor Fate, the Atom, Hawkman, Green Arrow, and Aquaman\", instead, the pipeline returned a different answer.  You can even try different question wordings:\n",
        "\n",
        "*   \"What superheroes were introduced between 1939 and 1941?\"\n",
        "*   \"What comic book characters were created between 1939 and 1941?\"\n",
        "*   \"What well-known characters were created between 1939 and 1941?\"\n",
        "*   \"What well-known superheroes were introduced between 1939 and 1941 by Detective Comics?\"\n",
        "\n",
        "and you will only get incorrect answers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91kLn9VcRzK"
      },
      "source": [
        "questions = [\"What popular superheroes were introduced between 1939 and 1941?\",\n",
        "             \"What superheroes were introduced between 1939 and 1941 by Detective Comics and its sister company?\",\n",
        "             \"What comic book characters were created between 1939 and 1941?\",\n",
        "             \"What well-known characters were created between 1939 and 1941?\",\n",
        "             \"What well-known superheroes were introduced between 1939 and 1941 by Detective Comics?\"]\n",
        "\n",
        "results = question_answerer(question=questions, context=context)\n",
        "\n",
        "for q, r in zip(questions, results):\n",
        "    print(q, \"\\n>> \" + r['answer'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCkLhf27cEsH"
      },
      "source": [
        "It seems like this model is a **huge fan** of Archie Andrews. It even considers him a superhero! \n",
        "\n",
        "The example that fooled your `question_answerer` belongs to the [TyDi QA dataset](https://ai.google.com/research/tydiqa), a dataset from Google for question/answering in diverse languages. To achieve better results when you know that the pipeline isn't working as it should, you need to consider fine-tuning your model.\n",
        "\n",
        "In the next ungraded lab, you will get the chance to fine-tune the DistilBert model using the TyDi QA dataset. \n",
        "\n"
      ]
    }
  ]
}